{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Authorship Identification for Comparing Your Posts vs Infamous CEOs'\n",
        "\n",
        "![Image: image from https://trekkingwithdennis.com/tag/star-trek-voyager/](banner.png \"image from trekkingwithdennis.com\")\n",
        "\n",
        "In this article, we will entertian ourselves by comparing our, and, our galatic industry leaders' posts on the professional network BeamedIn, againts those of past earthling CEOs who were either frauds or unnanimously declared as unpleasant people. \n",
        "\n",
        "We will do this with a technique called **Authorship Identification** in NLP. This enables us to identify the most likely author of articles, news or messages. \n",
        "\n",
        "# Building The Author Learning Pipeline\n",
        "\n",
        "Here are the steps we will undertake:\n",
        "1. Clean the dataset of CEO messages.\n",
        "2. Vectorize words\n",
        "3. Extract features through bag of word (BoW) or Latent Semantic Analysis (LSA).\n",
        "4. Train classifier - for this we will find the best from a group of classifiers.\n",
        "\n",
        "Let's prepare our notebook before this work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-05T16:15:49.720095Z",
          "iopub.status.busy": "2022-05-05T16:15:49.719366Z",
          "iopub.status.idle": "2022-05-05T16:16:27.390641Z",
          "shell.execute_reply": "2022-05-05T16:16:27.389477Z",
          "shell.execute_reply.started": "2022-05-05T16:15:49.720042Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publication</th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31675</th>\n",
              "      <td>tweets</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>Just learned that Jon @ Ossoff, who is running...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41303</th>\n",
              "      <td>tweets</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>With Votes in the House tomorrow, Democrats wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13150</th>\n",
              "      <td>tweets</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>\"To become a champion, fight one more round.\" ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29651</th>\n",
              "      <td>tweets</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>Another attack, this time in Germany. Many kil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6425</th>\n",
              "      <td>tweets</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>@ rubletz True--isn't it sad!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      publication        author  \\\n",
              "31675      tweets  Donald Trump   \n",
              "41303      tweets  Donald Trump   \n",
              "13150      tweets  Donald Trump   \n",
              "29651      tweets  Donald Trump   \n",
              "6425       tweets  Donald Trump   \n",
              "\n",
              "                                                 content  \n",
              "31675  Just learned that Jon @ Ossoff, who is running...  \n",
              "41303  With Votes in the House tomorrow, Democrats wa...  \n",
              "13150  \"To become a champion, fight one more round.\" ...  \n",
              "29651  Another attack, this time in Germany. Many kil...  \n",
              "6425                       @ rubletz True--isn't it sad!  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pip below for Kaggle and online notebooks.\n",
        "# !pip install ipywidgets\n",
        "# !pip install sklearn\n",
        "# !pip install spacy\n",
        "\n",
        "# General-purpose Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import re\n",
        "from collections import Counter\n",
        "%matplotlib inline\n",
        "\n",
        "# Remove anywarning texts from notebooks.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Discover files in kaggle if any.\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "data = pd.read_csv('./communications.csv')\n",
        "data = data.dropna()\n",
        "data.sample(5, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be using `scikit-learn` to create text processing pipelines. A Pipeline will create a compound classifier through these steps:\n",
        "1. vectorizer\n",
        "2. transformer\n",
        "3. classifier\n",
        "\n",
        "But first, we need to explore our data. We can look at the communication peices sizes, frequency of words or general sentiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAEVCAYAAACSSPCDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUz0lEQVR4nO3dfbBkdX3n8ffHmREIw8MqSjCKA4IxKgRxRFmNDD6sUcgaohEtdnXMA5VEE/NgpUC2XEiV2cFdqSSFkSULwqoQSnGjJQElyKixFDIDQ2ZQBoIZDAULkg0MICAM3/2jz2UOl3vn9tx7+/adH+9XVdc9/Tunz/n2tyg+8zvndHeqCkmSWvOMcRcgSdIoGHCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkp4iSSU5ZNx1SHNhwElPc0nWJvmNcdchzTcDTtLIJFk67hr09GXASY1IckqSW5Pcn+R7SU7oxk9P8tnediu6U5BLk3wM+AXg7CQPJDm7t8s3Jbklyb8l+WSSdK9/RpL/kuS2JHcn+d9J9pm0719P8kPg6wvXAenJDDipHbcyCKt9gDOAzyY5YEcvqKrTgG8BH6yq5VX1wd7q44FXAT8PvAt4Sze+unscCxwMLAf6wQhwDPBzvddIC86AkxpRVZ+vqjuq6vGqugS4BThqDrtcU1X3VtUPgauBI7rxk4CzquoHVfUAcCrw7kmnI0+vqger6qE5HF+aEwNOakSS9ybZkOTeJPcCLwf2m8Mu/29v+ccMZmoAzwNu6627DVgK7N8b+5c5HFeaFwac1IAkLwT+Cvgg8Oyq2hfYBAR4EPip3uY/PenlO/uTIncAL+w9PxB4DLhrDvuU5p0BJ7VhTwah8iOAJO9nMIMD2AC8PsmB3c0gp0567V0MrqUN62LgD5IclGQ58KfAJVX12Bzql+adASc1oKq+B3wC+A6DwDoM+Ha37krgEuAfgfXAVya9/M+Bd3Z3S/7FEIc7H/gM8E3gn4GHgd+dh7chzav4g6eSpBY5g5MkNcmAkyQ1yYCTJDXJgJMkNcmAkyQ1yYCTJDXJgJMkNcmAkyQ1yYCTJDXJgJMkNcmAkyQ1yYCTJDXJgJMkNcmAkyQ1yYCTJDXJgJMkNcmAkyQ1aem4C9B2++67bx1yyCHjLmNRe/DBB9lzzz3HXcaiZ59mZo+Gsyv0af369fdU1XMmjxtwi8j+++/PunXrxl3GorZ27VpWrVo17jIWPfs0M3s0nF2hT0lum2rcU5SSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCalqsZdgzoHHnxIPeNdfz7uMha1PzrsMT6x0d/pnYl9mpk9Gs5C9GnLmuPm9Pok66tq5eRxZ3CSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJo004JJsS7IhyY1Jbkjyh0nm/ZhJ1iZ5yk8lJFmd5OxJY+/vatqQ5CdJNnbLa+a7LknS+Iz6x5AeqqojAJI8F7gI2Af4ryM+7rSq6tPAp7uatgDHVtU9/W2SLKmqbWMoT5I0TxbsFGVV3Q2cDHwwA7sn+XQ3g7o+ybHwxKzri0muSHJLko9P7CPJp5Ks62aEZ0x1nG6GdnOSbwCvHba+JA8k+ZMk1wBHJ9mSZL9u3coka7vl05NcmORr3Ta/kuTj3fu4IsmybrstSc5Mcm33OGSWrZMkzcKCXoOrqh90x3wu8IFu7DDgPcCFSXbvNj0COBE4DDgxyQu68dO6X209HDgmyeH9/Sc5ADiDQbC9GXjpTpS3J7Cpql5dVX8/w7YvAo4D3g58Fri6ex8PdeMTtlbVUcDZwJ/tRC2SpDkax00m6f6+DvgMQFXdBNwGvLhbd1VV3VdVDwPfA17Yjb8ryXXA9cDLeGqAvRpYW1U/qqqfAJfsRF3bgEuH3PbyqnoU2AgsAa7oxjcCK3rbXdz7e/RUO0pycjcrXffA1q07Ua4kaUcWNOCSHMwgSO5me9BN5ZHe8jZgaZKDgA8Db6yqw4HLgN2neG3NsryHJ113e4zt/Zl8nEcAqupx4NGqmjjm4zz5umZNs7x9sOrcqlpZVSuX7733LEuXJE22YAGX5DnAOcDZXSB8EzipW/di4EBg8w52sTfwIHBfkv2Bt06xzTXAqiTP7q6F/eocSt4CvLJbfscs93Fi7+935lCLJGknjfouyj2SbACWMZgRfQY4q1v3l8A5STZ261ZX1SPJ1BO7qrohyfXAjcAPgG9Psc2dSU5nECZ3AtcxOIU4G2cA5yX5CIPgnI3duptWnsHgOqMkaYGMNOCqatpw6a6vrZ5i/ALggt7z43vLT9m+G1/VW37iYwBD1Leit7x80rpvsf2aYH/89EnPl0+3DvhkVU15t6ckabT8JhNJUpNGfYryaas/O5QkLTxncJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQm+V2Ui8gey5awec1x4y5jUVu7di1bTlo17jIWPfs0M3s0nF25T87gJElNMuAkSU0y4CRJTTLgJElNMuAkSU0y4CRJTTLgJElNMuAkSU0y4CRJTTLgJElNMuAkSU0y4CRJTTLgJElNMuAkSU0y4CRJTTLgJElNMuAkSU0y4CRJTTLgJElNMuAkSU0y4CRJTTLgJElNMuAkSU0y4CRJTTLgJElNMuAkSU0y4CRJTTLgJElNMuAkSU0y4CRJTTLgJElNMuAkSU0y4CRJTTLgJElNMuAkSU1aOu4CtN1Dj25jxSmXLfhxt6w5bsGPKUmj5gxOktQkA06S1CQDTpLUpBkDLsmSJH+3EMVIkjRfZgy4qtoG/DjJPgtQjyRJ82LYuygfBjYmuRJ4cGKwqn5vJFVJkjRHwwbcZd1DkqRdwlABV1UXJnkm8OJuaHNVPTq6siRJmpuhAi7JKuBCYAsQ4AVJ3ldV3xxZZZIkzcGwpyg/AfyHqtoMkOTFwMXAK0dVmCRJczHs5+CWTYQbQFXdDCwbTUmSJM3dsDO4dUnOAz7TPT8JWD+akiRJmrthA+63gQ8Av8fgGtw3gb8cVVGSJM3VsHdRPgKc1T0kSVr0hroGl+S1Sa5McnOSH0w8Rl3cpBq2JdnQe5zSja9NsnIe9r86SSV5Y2/shG7snbPY36okX5lrXZKk2Rn2FOV5wB8wuO62bXTl7NBDVXXEiI+xEXgPcFX3/N3ADSM+piRpBIa9i/K+qrq8qu6uqn+deIy0sllI8p4kG5NsSnJmb/yBJB9LckOS7ybZf5pdfAs4KsmyJMuBQ4ANvf1sSbJft7wyydpu+ZjezPL6JHtNqutV3fjB8/uOJUnT2WHAJTkyyZHA1Un+e5KjJ8a68YW0x6RTlCdOqvV5wJnAG4AjgFcl+eVu9Z7Ad6vq5xncIPOb0xyjgL8D3gK8HfjykLV9GPhAN8P8BeChXl3/HjgHeHtVPeW0bpKTk6xLsu6BrVuHPJwkaSYznaL8xKTn/WtdxSBMFspMpyhfBaytqh8BJPkc8Hrgb4CfABPXw9YDb97Bfv6awd2i+wB/BHxkiNq+DZzVHfOLVXV7EoCfA85l8CH5O6Z6YVWd223DgQcfUkMcS5I0hB0GXFUdC5Dk4Mmzj0V4ui07WPdoVU2ExzZ28L6r6tokL2cQqDd3QTXhMbbPenfvvWZNksuAtwHfTfKmbtWd3XavAKYMOEnSaAx7De4LU4x9fj4LmQfXAMck2S/JEgY3i3xjlvs6lalnblvY/vVk75gYTPKiqtpYVWcC64CXdKvuBY4D/rT7Pk9J0gLZ4QwuyUuAlwH7JPmV3qq96c1gFsgeSTb0nl9RVadMPKmqO5OcClzNYDb3t1X1pdkcqKoun2bVGcB5ST7CIFAn/H6SYxnMDr8HXA4c3e3rriS/BFye5Neq6prJO5Ukzb+ZrsH9LHA8sC/wS73x+5n+Ro2RqKol04yv6i1fBFw0xTbLe8tfYIoZaVVdAFwwxfjq3vK32P6TQf1tfneK0tZ2D6rqhwz+oSBJWiAzXYP7EvClJEdX1XcWqCZJkuZs2A96n5zkKTO2qvq1ea5HkqR5MWzA9b9yanfgBLwrUJK0iA37ZcuX9p8nuZjBB6IlSVqUhv2YwGSHAgfOZyGSJM2noWZwSe5n8M0ldH/vAv54VEVJkjRXw56i3CvJsxjM3CY+/+bXSkmSFq1hZ3C/AXwIeD6Db9d/DfAdFva7KCVJGtqw1+A+xODLjG/rvp/yFcCPRlaVJElzNGzAPVxVDwMk2a2qbmLwLSeSJC1Kw34O7vYk+zL46Zkrk/wbfg5OkrSIDXuTyQnd4ulJrmbwW2lXjKwqSZLmaNgZ3BOqarY/QaMZ7LFsCZvXHDfuMiSpCbP9oLckSYuaASdJapIBJ0lqkgEnSWqSASdJapIBJ0lqkgEnSWqSASdJapIBJ0lqkgEnSWqSASdJapIBJ0lqkgEnSWqSASdJapIBJ0lqkgEnSWqSASdJapIBJ0lqkgEnSWqSASdJapIBJ0lqkgEnSWqSASdJapIBJ0lqkgEnSWqSASdJapIBJ0lqkgEnSWqSASdJapIBJ0lqkgEnSWqSASdJapIBJ0lqkgEnSWqSASdJatLScReg7R56dBsrTrlsZPvfsua4ke1bkhYbZ3CSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJhlwkqQmGXCSpCYZcJKkJo0s4JI8MOn56iRnd8u/leS93fIFSd7ZLa9NsnJE9Xw4yU1JNiW5oXf8J46Z5G+T7DvDflYned4oapQkzZ+x/B5cVZ0zH/tJsqSqtg2x3W8BbwaOqqqtSfYBfnmKut42xGFXA5uAO3auWknSQhrLKcokpyf58AzbfCrJuiQ3JjmjN74lyUeT/D1wSpLreusOTbJ+it19BPidqtoKUFX3VdWFUxxzS5L9kqxI8v0kf9Ud/2tJ9uhmmiuBzyXZ0I29Mcn1STYmOT/Jbr19nZHkum7dS2bVLEnSrIwy4PboQmBDkg3An+zk60+rqpXA4cAxSQ7vrXu4ql5XVR8D7ktyRDf+fuCC/k6S7AXsVVW37uTxDwU+WVUvA+4F3lFVXwDWASdV1RFAdcc7saoOYzAj/u3ePu6pqiOBTwE7DHRJ0vwaZcA9VFVHTDyAj+7k69/Vzc6uB14GvLS37pLe8v8C3p9kCXAicNGk/YRBEO2sf66qDd3yemDFFNv8bLfdzd3zC4HX99Z/cYbXk+Tkbqa67oGtW2dRpiRpKovyLsokBzGY8byxqg4HLgN2723yYG/5UuCtwPHA+qr61/6+utOSDyY5eCfLeKS3vI2pr1dmyH1M93qq6tyqWllVK5fvvfdOlihJms6iDDhgbwYhdl+S/RkE2JSq6mHgqwxOA356ms3+G/DJJHsDJNk7ycmzrO1+YK9u+SZgRZJDuuf/GfjGLPcrSZpHizLgquoGBqcmbwTOB749w0s+x+A05NemWf8p4GrgH5JsYhBCP55leRcA53TXFcPgut/nk2wEHgfm5Q5RSdLcjOxjAlW1fNLzC+huAKmq03vjq3vLq6Yan7SfFVMMvw44f7qPDFRVAR/vHpPX9Y85se97gJf3xv9Hb/lSBqdFJ1wFvGJHdVbVOmDV5G0kSaMzls/Bzack/wd4EfCGcdciSVo8dvmAq6oTxl2DJGnxWZTX4CRJmisDTpLUJANOktQkA06S1CQDTpLUJANOktQkA06S1CQDTpLUJANOktQkA06S1CQDTpLUpF3+uyhbsseyJWxec9y4y5CkJjiDkyQ1yYCTJDXJgJMkNcmAkyQ1yYCTJDXJgJMkNcmAkyQ1yYCTJDXJgJMkNcmAkyQ1yYCTJDXJgJMkNcmAkyQ1yYCTJDXJgJMkNcmAkyQ1yYCTJDUpVTXuGtRJcj+wedx1LHL7AfeMu4hdgH2amT0azq7QpxdW1XMmDy4dRyWa1uaqWjnuIhazJOvs0czs08zs0XB25T55ilKS1CQDTpLUJANucTl33AXsAuzRcOzTzOzRcHbZPnmTiSSpSc7gJElNMuAWgSS/mGRzkn9Kcsq461kISc5PcneSTb2xZyW5Mskt3d9/11t3atefzUne0ht/ZZKN3bq/SJJufLckl3Tj1yRZsaBvcB4keUGSq5N8P8mNST7UjdunTpLdk1yb5IauR2d04/ZokiRLklyf5Cvd8/Z7VFU+xvgAlgC3AgcDzwRuAF467roW4H2/HjgS2NQb+zhwSrd8CnBmt/zSri+7AQd1/VrSrbsWOBoIcDnw1m78d4BzuuV3A5eM+z3PokcHAEd2y3sBN3e9sE/bexRgebe8DLgGeI09mrJXfwhcBHyle958j8ZewNP90f3H8tXe81OBU8dd1wK99xWTAm4zcEC3fACDzwU+pSfAV7u+HQDc1Bt/D/A/+9t0y0sZfFA1437Pc+zXl4A326dp+/NTwHXAq+3RU3rzfOAq4A29gGu+R56iHL+fAf6l9/z2buzpaP+quhOg+/vcbny6Hv1Mtzx5/EmvqarHgPuAZ4+s8hHrTvm8gsEMxT71dKfeNgB3A1dWlT16qj8D/hh4vDfWfI8MuPHLFGPe2vpk0/VoR71rpq9JlgOXAr9fVVt3tOkUY833qaq2VdURDGYpRyV5+Q42f9r1KMnxwN1VtX7Yl0wxtkv2yIAbv9uBF/SePx+4Y0y1jNtdSQ4A6P7e3Y1P16Pbu+XJ4096TZKlwD7A/xtZ5SOSZBmDcPtcVX2xG7ZPU6iqe4G1wC9ij/peC/zHJFuAvwbekOSzPA16ZMCN3z8AhyY5KMkzGVyg/fKYaxqXLwPv65bfx+Ca08T4u7s7tQ4CDgWu7U6r3J/kNd3dXO+d9JqJfb0T+Hp1Fwh2Fd17Og/4flWd1VtlnzpJnpNk3255D+BNwE3YoydU1alV9fyqWsHg/y9fr6r/xNOhR+O+COijAN7G4A65W4HTxl3PAr3ni4E7gUcZ/Ovv1xmcs78KuKX7+6ze9qd1/dlMd+dWN74S2NStO5vtX16wO/B54J8Y3Pl18Ljf8yx69DoGp3n+EdjQPd5mn57Uo8OB67sebQI+2o3bo6n7tYrtN5k03yO/yUSS1CRPUUqSmmTASZKaZMBJkppkwEmSmmTASZKaZMBJkppkwEmSmmTASZKa9P8B8/hlTAMMsQMAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wordcloud\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.suptitle(\"author\", fontsize=12)\n",
        "data[\"author\"].reset_index().groupby(\"author\").count().sort_values(by=\"index\").plot(\n",
        "  kind=\"barh\", \n",
        "  legend=False,\n",
        "  ax=ax).grid(axis='x')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing to the number of peices in each corpus, we see our dataset is unbalanced.\n",
        "A resampling should be done to normalize the dataset, preferably with a cleanup:\n",
        "- Drop stopwords (e.g. and, are, what)\n",
        "- Lemmatize (e.g. driven will become drive, as driver will result in drive)\n",
        "- Drop common words (i.e. beyond a median frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resampled and Normalized\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "min_rows = data[\"author\"].reset_index().groupby(\"author\").count().min().values[0]\n",
        "# min_rows = 50\n",
        "\n",
        "authors = data[\"author\"].unique()\n",
        "n_df = pd.DataFrame(columns =['author', 'content'])\n",
        "for author in authors:\n",
        "  row = data[data[\"author\"] == author].sample(n=min_rows, random_state=1)\n",
        "  n_df = n_df.append(row[['author', 'content']], ignore_index = True)\n",
        "n_df = n_df.dropna()\n",
        "\n",
        "# Tokenize, lemmatize and drop stop words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from collections import Counter\n",
        "\n",
        "sentim = SentimentIntensityAnalyzer()\n",
        "stops = set(stopwords.words('english'))\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "n_df['n_content'] = n_df[\"content\"].apply(\n",
        "  lambda x: \n",
        "    ' '.join([wnl.lemmatize(token) for token in word_tokenize(x) \n",
        "    if token not in stops])\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze length, counts, etc.\n",
        "n_df['word_count'] = n_df[\"n_content\"].apply(lambda x: len(str(x).split(\" \")))\n",
        "n_df['char_count'] = n_df[\"n_content\"].apply(\n",
        "    lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
        "\n",
        "x, y = \"word_count\", \"author\"\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
        "fig.suptitle(x)\n",
        "for i in authors:\n",
        "    sns.distplot(n_df[n_df[y] == i][x], hist=True, kde=False,\n",
        "                 bins=10, hist_kws={\"alpha\": 0.8},\n",
        "                 axlabel=\"histogram\", ax=ax[0])\n",
        "    sns.distplot(n_df[n_df[y] == i][x], hist=False, kde=True,\n",
        "                 kde_kws={\"shade\": True}, axlabel=\"density\",\n",
        "                 ax=ax[1])\n",
        "ax[0].legend(authors)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a visible density in the area of 10 to 30 words. With this knowledge, we can use to further balance our dataset by sampling only peices that have an average amount of words that fall in this range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_df = pd.DataFrame(columns=['author', 'content', 'n_content'])\n",
        "for author in authors:\n",
        "  row = n_df[(n_df[\"author\"] == author) & (n_df[\"word_count\"] >= 10) & (n_df[\"word_count\"] <= 30)]\n",
        "  word_df = word_df.append(row[['author', 'content', 'n_content']], ignore_index=True)\n",
        "n_df = word_df.dropna()\n",
        "print(n_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "n_df[\"sentiment\"] = n_df['n_content'].apply(\n",
        "  lambda x:\n",
        "    sentim.polarity_scores(x)['compound'])\n",
        "\n",
        "n_df[\"tags\"] = n_df['n_content'].apply(\n",
        "  lambda x: \n",
        "    [t.label() for t in ne_chunk(pos_tag(\n",
        "      x.split(' '))) if hasattr(t, 'label')])\n",
        "    \n",
        "TAG_SET = list(\n",
        "    set([tag for tags in n_df[\"tags\"] for tag in tags]))\n",
        "for tag in TAG_SET:\n",
        "    n_df[tag] = n_df['tags'].map(\n",
        "        lambda x: x.count(tag) if x is not None and len(x) >= 0 else 0)\n",
        "n_tags = pd.DataFrame(columns=TAG_SET)\n",
        "n_df= n_df.drop(columns=[\"tags\"])\n",
        "\n",
        "sns.set(style=\"white\")\n",
        "for author in authors:\n",
        "  temp = pd.DataFrame(n_df[n_df[\"author\"] == author][TAG_SET].sum()).T\n",
        "  temp['author'] = author\n",
        "  n_tags = n_tags.append(temp)\n",
        "temp = n_tags.melt(id_vars=\"author\", var_name=\"PoS\", value_name=\"Freq\")\n",
        "sns.factorplot(x='Freq', y=\"PoS\", hue=\"author\", data=temp,\n",
        "               kind='bar', size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using NER we identified what most of earth's CEOs used to ramble on in their communications. Standard NER codes are:\n",
        "1.\tORGANIZATION e.g. Microsoft, Facebook\n",
        "2.\tPERSON e.g. Rafael Nadal, Nelson Mandela\n",
        "3.\tMONEY e.g. 9 million dollars, INR 4 Crore\n",
        "4.\tGPE e.g. India, Australia, South East Asia\n",
        "1.  GSP is a Geo-Socio-Political group\n",
        "5.\tLOCATION e.g. Mount Everest, River Ganga\n",
        "6.\tDATE e.g. 9th May 1987, 4 AUG\n",
        "7.\tTIME e.g. 7:23 A.M., three-forty am\n",
        "\n",
        "Even better, let's see if their is corelation between the mood (or Sentiment) of the communication peice and the entities mentioned therein:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp = pd.DataFrame(columns=[*TAG_SET, *[\"author\"]])\n",
        "for author in authors:\n",
        "  for tag in TAG_SET:\n",
        "    sent = (n_df[(n_df[\"author\"] == author) & (\n",
        "        n_df[tag] >= 1)][\"sentiment\"].mean())\n",
        "    temp.at[author, tag] = sent if sent > 0 else 0\n",
        "  temp.at[author, \"author\"] = author\n",
        "temp = temp.dropna()\n",
        "\n",
        "temp = temp.melt(id_vars=\"author\", var_name=\"PoS\", value_name=\"Sentiments\")\n",
        "sns.factorplot(x='Sentiments', y=\"PoS\", hue=\"author\", data=temp,\n",
        "               kind='bar', size=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, a word cloud will allow us to explore the words themselves (lemmatized here) and their frequency in relation to the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in authors:\n",
        "  print(f'Wordcloud for: {i}')\n",
        "  \n",
        "  wc = wordcloud.WordCloud(background_color='black', max_words=200,\n",
        "                           max_font_size=80, width=800, height=600)\n",
        "\n",
        "  corpus = n_df[n_df[\"author\"] == author]['n_content']\n",
        "  wc = wc.generate(text=str(corpus))\n",
        "  plt.imshow(wc, cmap=None, interpolation='bilinear')\n",
        "  \n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's build the model.\n",
        "\n",
        "We will use the vectors derived from the corpus to act as features for our model. Features which we will apply with a stacking classifier to, which will combine the best outputs of each classifier to find an optimal prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.stem.porter import *\n",
        "import nltk\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# nltk.download()\n",
        "\n",
        "# Vectorizing on normalized text above.\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "lgc = LogisticRegression(n_jobs=-1, max_iter=1000)\n",
        "sgdc = SGDClassifier(loss='hinge',\n",
        "                     penalty='l2',\n",
        "                     alpha=1e-3,\n",
        "                     random_state=RANDOM_STATE,\n",
        "                     tol=None,\n",
        "                     n_jobs=-1)\n",
        "estimators = [('sgdc', sgdc), ('lgc', lgc)]\n",
        "etc = ExtraTreesClassifier(random_state=RANDOM_STATE)\n",
        "sclf = StackingClassifier(estimators=estimators,\n",
        "                          final_estimator=lgc,\n",
        "                          passthrough=True)\n",
        "tfidf = TfidfTransformer(use_idf=False)\n",
        "\n",
        "# Pipeline\n",
        "n_df = n_df.dropna()\n",
        "y = n_df['author']\n",
        "X = n_df['n_content']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.24,\n",
        "                                                    random_state=RANDOM_STATE,\n",
        "                                                    stratify=y)\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'clf__sgdc__alpha': (1e-2, 1e-3),\n",
        "    'clf__sgdc__tol': (0.0, 1e-3),\n",
        "    'clf__lgc__solver': ['lbfgs', 'sag'],\n",
        "    'clf__lgc__C': [0.2, 0.7],\n",
        "    'clf__lgc__penalty': ['c', 'l2']\n",
        "}\n",
        "\n",
        "# To find out for the param grid.\n",
        "# print(text_clf.get_params())\n",
        "text_clf = Pipeline([\n",
        "    ('vect', vect),\n",
        "    ('tfidf', tfidf),\n",
        "    ('clf', sclf),\n",
        "])\n",
        "gs_clf = GridSearchCV(estimator=text_clf,\n",
        "                      param_grid=parameters,\n",
        "                      cv=5, n_jobs=-1)\n",
        "gs_clf = gs_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vetting the models we stacked:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted = gs_clf.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, predicted,\n",
        "                                     target_names=y.unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above 80% is good enough. SKTLearn has a recommendation map to guide you to the most optimal model:\n",
        "![Image: image from sktlearn](ml_map.png \"image from tsktlearn\")\n",
        "\n",
        "Finally, let's test it on some samples from this same article to find to which bad CEO our text's style mostly resembles, and by how much:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"In this article, we will entertian ourselves by comparing our's, and, our galatic leaders' posts on the professional network BeamedIn, againts those of past earthling CEOs who were either frauds or unnanimously declared as unpleasant people. We will do this with a technique called ** Authorship Identification ** in NLP. This enables us to identify the most likely author of articles, news or messages. \"\n",
        "toks = [wnl.lemmatize(token) for token in word_tokenize(text) if token not in stops]\n",
        "text_predicted = gs_clf.predict(toks)\n",
        "text_predicted_prob = gs_clf.predict_proba(toks)\n",
        "\n",
        "txt = text_predicted[0]\n",
        "txt_idx = np.where(authors == txt)\n",
        "\n",
        "probability = text_predicted_prob[0][txt_idx][0]\n",
        "print(f'This text is something \"{txt}\" would write at {probability:.0%}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Concluding our Analysis\n",
        "\n",
        "\n",
        "\n",
        "# References\n",
        "\n",
        "- https://www.nltk.org/api/nltk.classify.scikitlearn.html\n",
        "- https://scikit-learn.org/stable/\n",
        "- https://www.one37pm.com/popular-culture/biggest-ceo-frauds-in-history\n",
        "\n",
        "## Github and Kaggle\n",
        "\n",
        "Article here is also available on [Github](https://github.com/adamd1985/articles/blob/main/nlp_bs_meter/nlp_intro.ipynb) and [Kaggle](https://www.kaggle.com/code/addarm/authorship-identification-you-vs-ceos)\n",
        "\n",
        "#\n",
        "<div align=\"right\">Made with :heartpulse: by <b>Adam</b></div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "7923e494469ea52b8a9df08e257af4ba4a4bf82d7b34513bcf6fc594ca6701f8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
