{"cells":[{"cell_type":"markdown","metadata":{},"source":["# NLP Authorship Identification - Your Social Media Posts vs Bad CEOs'\n","\n","![Image: image from https://trekkingwithdennis.com/tag/star-trek-voyager/](banner.png \"image from trekkingwithdennis.com\")\n","\n","In this article, we will entertian ourselves by comparing our's, and, our galatic leaders' posts on the professional network BeamedIn, againts those of past earthling CEOs who were convicted frauds or unnanimously declared as unpleasant folk. \n","\n","We will do this with a technique called **Authorship Identification** in NLP.\n","\n","This enables us to identify the most likely author of articles, news or messages. Authorship identification. This will be our aegies in navigating the multitude of Star Trek level of villinous propaganda in this age of social media\n","\n","# Building The Author Learning Pipeline\n","\n","Here are the steps we will undertake:\n","1. Clean these articles: stop words, lematize, and normalize.\n","2. Extract features through bag of word (BoW).\n","3. Tokenize words\n","4. Downscale to frequencies from word occurences.\n","5. Train classifier - for this we will find the best from a group of classifiers.\n","\n","Let's prepare our notebook before the above work:"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-05-05T16:15:49.720095Z","iopub.status.busy":"2022-05-05T16:15:49.719366Z","iopub.status.idle":"2022-05-05T16:16:27.390641Z","shell.execute_reply":"2022-05-05T16:16:27.389477Z","shell.execute_reply.started":"2022-05-05T16:15:49.720042Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>publication</th>\n","      <th>author</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>messages</td>\n","      <td>Elizabeth Holmes</td>\n","      <td>‘I want to be a billionaire.’\\n‘No, the presid...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  publication            author  \\\n","0    messages  Elizabeth Holmes   \n","\n","                                             content  \n","0  ‘I want to be a billionaire.’\\n‘No, the presid...  "]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["# Pip below for Kaggle and online notebooks.\n","# !pip install ipywidgets\n","# !pip install sklearn\n","# !pip install spacy\n","\n","# General-purpose Libraries\n","import os\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","from collections import Counter\n","%matplotlib inline\n","\n","# Remove anywarning texts from notebooks.\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Discover files in kaggle if any.\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","data = pd.read_csv('./communications.csv')\n","data.head(1)"]},{"cell_type":"markdown","metadata":{},"source":["We will be using `scikit-learn` to create pipelines. A Pipeline will create a compound classifier through these steps:\n","1. vectorizer\n","2. transformer\n","3. classifier"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"]}],"source":["# 1\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from nltk.stem.snowball import SnowballStemmer\n","from sklearn.linear_model import LogisticRegression\n","from nltk.stem.porter import *\n","import nltk\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import CountVectorizer\n","nltk.download()\n","\n","stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n","class StemmedCountVectorizer(CountVectorizer):\n","    def build_analyzer(self):\n","        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n","        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n","\n","stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n","\n","# 2\n","RANDOM_STATE = 42\n","\n","nbc = MultinomialNB(fit_prior=False)\n","lgc = LogisticRegression(n_jobs=-1)\n","knc = KNeighborsClassifier(n_neighbors=1)\n","sgdc = SGDClassifier(loss='hinge',\n","                      penalty='l2',\n","                      alpha=1e-3,\n","                      random_state=42,\n","                      max_iter=5,\n","                      tol=None,\n","                      n_jobs=-1)\n","estimators = [('nbc', nbc), ('sgdc', sgdc), ('lgc', lgc)]\n","etc = ExtraTreesClassifier(random_state=RANDOM_STATE)\n","\n","# 3\n","kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n","sclf = StackingClassifier(estimators=estimators, \n","                          final_estimator=etc, \n","                          passthrough=True, \n","                          cv=kfold)\n","tfidf = TfidfTransformer(use_idf=False)\n","\n","# Pipeline\n","from sklearn import model_selection\n","from sklearn.pipeline import Pipeline\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import train_test_split\n","\n","data = data.dropna()\n","\n","y = data['author']\n","X = data['content']\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size=0.24,\n","                                                    random_state=0,\n","                                                    stratify=y)\n","\n","parameters = {\n","  'vect__ngram_range': [(1, 1), (1, 2)],\n","  'tfidf__use_idf': (True, False),\n","  'clf__sgdc__alpha': (1e-2, 1e-3),\n","  'clf__sgdc__max_iter': (5, 10),\n","  'clf__sgdc__tol': (0.0, 1e-3),\n","  'clf__lgc__solver': ['newton-cg', 'lbfgs', 'sag'],\n","  'clf__lgc__C': [0.3, 0.5, 0.7, 1],\n","  'clf__lgc__penalty': ['none', 'l2']\n","}\n","# To find out for the param grid.\n","# print(text_clf.get_params())\n","text_clf = Pipeline([\n","    ('vect', stemmed_count_vect),\n","    ('tfidf', tfidf),\n","    ('clf', sclf),\n","])\n","gs_clf = GridSearchCV(estimator=text_clf,\n","                      param_grid=parameters,\n","                      cv=5, n_jobs=-1)\n","gs_clf = gs_clf.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's vet the models we are stacking:"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                  precision    recall  f1-score   support\n","\n","Elizabeth Holmes       0.00      0.00      0.00         5\n","   Sunny Balwani       1.00      0.33      0.50         3\n","   Trevor Milton       0.48      1.00      0.65        11\n","    Adam Neumann       0.00      0.00      0.00         4\n","Jeffrey Skilling       0.00      0.00      0.00         1\n","    Donald Trump       0.80      0.80      0.80         5\n","\n","        accuracy                           0.55        29\n","       macro avg       0.38      0.36      0.32        29\n","    weighted avg       0.42      0.55      0.44        29\n","\n"]},{"data":{"text/plain":["0.5517241379310345"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["predicted = gs_clf.predict(X_test)\n","\n","from sklearn import metrics\n","print(metrics.classification_report(y_test, predicted,\n","                                     target_names=y.unique()))\n","\n","np.mean(predicted == y_test)"]},{"cell_type":"markdown","metadata":{},"source":["50% no perfect. We need more work here or specific models. Here is what SKTLearn recommends:\n","\n","ml_map.png\n","\n","Here we try some text to see to who it is"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"data":{"text/plain":["array([0.58])"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["test_data = {'author': [''], 'publication': [''], 'content': [\n","    'The indictment, in a lot of ways, that was the turning point.']}\n","text = pd.DataFrame(test_data)\n","\n","text_predicted = gs_clf.predict(text['content'])\n","text_predicted_prob = gs_clf.predict_proba(text['content'])\n","\n","y = np.array(data['author'].unique())\n","txt = text_predicted[0]\n","txt_idx = np.where(y == txt)\n","text_predicted_prob[0][txt_idx]"]},{"cell_type":"markdown","metadata":{},"source":["# Concluding our Analysis\n","\n","\n","\n","# References\n","\n","- https://spacy.io\n","\n","## Github and Kaggle\n","\n","Article here is also available on [Github]() and [Kaggle]()\n","\n","#\n","<div align=\"right\">Made with :heartpulse: by <b>Adam</b></div>"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"7923e494469ea52b8a9df08e257af4ba4a4bf82d7b34513bcf6fc594ca6701f8"}}},"nbformat":4,"nbformat_minor":4}
